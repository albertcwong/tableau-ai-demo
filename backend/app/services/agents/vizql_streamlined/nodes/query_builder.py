"""Enhanced query builder node with tool integration."""
import json
import logging
from typing import Dict, Any, List, Optional, Set
from datetime import datetime

from app.services.agents.vizql_streamlined.state import StreamlinedVizQLState
from app.services.agents.vizql_streamlined.tools import (
    get_datasource_schema,
    get_datasource_metadata,
    get_prior_query
)
from app.services.agents.vizql.context_builder import build_full_compressed_context
from app.prompts.registry import prompt_registry
from app.services.ai.client import UnifiedAIClient
from app.core.config import settings
from app.services.metrics import track_node_execution

logger = logging.getLogger(__name__)


def _detect_and_apply_date_functions(query_draft: Dict[str, Any], user_query: str, enriched_schema: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Automatically detect temporal grouping keywords and apply date functions to date fields.
    
    This is a programmatic fallback for when the LLM doesn't follow prompt instructions.
    
    Args:
        query_draft: The query generated by the LLM
        user_query: The user's original query text
        enriched_schema: Schema information to identify date fields
        
    Returns:
        Modified query with date functions applied where needed
    """
    user_query_lower = user_query.lower()
    
    # Detect temporal grouping keywords and map to functions
    temporal_patterns = {
        "trunc_month": ["by month", "monthly", "per month", "each month", "every month"],
        "trunc_year": ["by year", "yearly", "per year", "each year", "every year", "annually"],
        "trunc_quarter": ["by quarter", "quarterly", "per quarter", "each quarter", "every quarter"],
        "trunc_week": ["by week", "weekly", "per week", "each week", "every week"],
        "trunc_day": ["by day", "daily", "per day", "each day", "every day"]
    }
    
    # Check if any temporal keyword is present
    detected_function = None
    for func_name, keywords in temporal_patterns.items():
        if any(keyword in user_query_lower for keyword in keywords):
            detected_function = func_name.upper()
            logger.info(f"Detected temporal grouping keyword, will apply {detected_function} function")
            break
    
    # If no temporal keyword detected, return query as-is
    if not detected_function:
        return query_draft
    
    # Get date field names from schema
    date_field_names = set()
    if enriched_schema:
        for field in enriched_schema.get("fields", []):
            data_type = field.get("dataType", "").upper()
            if data_type in ["DATE", "DATETIME", "TIMESTAMP"]:
                date_field_names.add(field.get("fieldCaption"))
    
    # If we don't have schema info, use common date field patterns
    if not date_field_names:
        # Common date field patterns
        date_patterns = ["date", "time", "datetime", "timestamp", "created", "modified", "order date", "ship date", "purchase date"]
        date_field_names = set([p.title() for p in date_patterns] + [p.upper() for p in date_patterns] + ["Order Date", "Ship Date"])
    
    # Check and fix fields in the query
    fields = query_draft.get("query", {}).get("fields", [])
    modified = False
    
    for field in fields:
        field_caption = field.get("fieldCaption", "")
        field_function = field.get("function")
        
        # Check if this is a date field without a function
        is_date_field = any(date_name.lower() in field_caption.lower() for date_name in date_field_names) or field_caption in date_field_names
        
        if is_date_field and not field_function and "calculation" not in field:
            # Apply the detected temporal function
            field["function"] = detected_function
            logger.warning(
                f"ðŸ”§ AUTO-FIX: Added '{detected_function}' function to date field '{field_caption}' "
                f"(detected from user query: '{user_query}')"
            )
            modified = True
    
    if modified:
        logger.info(f"Applied automatic date function fixes to query")
    
    return query_draft


def _detect_and_apply_count_functions(query_draft: Dict[str, Any], user_query: str, enriched_schema: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Automatically detect "how many" queries and apply COUNTD function.
    
    This is a programmatic fallback for when the LLM doesn't follow prompt instructions
    about counting distinct entities.
    
    Args:
        query_draft: The query generated by the LLM
        user_query: The user's original query text
        enriched_schema: Schema information to identify fields
        
    Returns:
        Modified query with COUNTD functions applied where needed
    """
    user_query_lower = user_query.lower()
    
    # Patterns that indicate count/distinct queries
    count_patterns = ["how many", "count of", "number of", "count distinct", "unique count"]
    
    # Check if any count pattern is present
    needs_count = any(pattern in user_query_lower for pattern in count_patterns)
    
    if not needs_count:
        return query_draft
    
    # Get fields from query
    fields = query_draft.get("query", {}).get("fields", [])
    
    # Check if query has exactly 1 field without a function
    # This is the pattern: "how many customers" â†’ field without function
    if len(fields) == 1:
        field = fields[0]
        field_caption = field.get("fieldCaption", "")
        has_function = "function" in field
        
        if not has_function and field_caption:
            # Apply COUNTD function
            field["function"] = "COUNTD"
            logger.warning(
                f"ðŸ”§ AUTO-FIX: Added 'COUNTD' function to field '{field_caption}' "
                f"(detected 'how many' pattern in user query: '{user_query}')"
            )
            return query_draft
    
    # Check for fields that match the entity being counted
    # E.g., "how many customers" with field "Customer Name" without function
    for field in fields:
        field_caption = field.get("fieldCaption", "")
        has_function = "function" in field
        
        if not has_function and field_caption:
            # Check if field name is mentioned in the query
            field_words = field_caption.lower().split()
            
            # Check if user query contains the field entity
            # E.g., "how many customers" and field is "Customer Name"
            for word in field_words:
                # Common entity names
                if word in ["customer", "order", "product", "region", "category", "state", "city", "country"]:
                    # Check if this word appears after "how many" or "count of"
                    for pattern in count_patterns:
                        pattern_idx = user_query_lower.find(pattern)
                        if pattern_idx >= 0:
                            # Check if the word appears near the pattern
                            query_after_pattern = user_query_lower[pattern_idx:]
                            if word in query_after_pattern:
                                field["function"] = "COUNTD"
                                logger.warning(
                                    f"ðŸ”§ AUTO-FIX: Added 'COUNTD' function to field '{field_caption}' "
                                    f"(detected '{pattern} {word}' in user query)"
                                )
                                return query_draft
    
    return query_draft


def _detect_and_apply_context_filters(query_draft: Dict[str, Any], user_query: str) -> Dict[str, Any]:
    """
    Automatically detect hierarchical filter dependencies and apply context:true appropriately.
    
    This is a programmatic fallback for when the LLM doesn't follow prompt instructions
    about context filters.
    
    Context filters are needed when one filter establishes a scope for another filter.
    Example: "given the top region, show me the top 3 customers"
    - First filter (top region) needs context:true
    - Second filter (top 3 customers) operates within that context
    
    Args:
        query_draft: The query generated by the LLM
        user_query: The user's original query text
        
    Returns:
        Modified query with context:true applied where needed
    """
    user_query_lower = user_query.lower()
    
    # Linguistic patterns that indicate hierarchical dependencies
    hierarchical_patterns = [
        # Pattern: "given X, show/find Y"
        (r"given\s+(?:the\s+)?(?:top|best|largest|highest|biggest)\s+(\d+\s+)?(\w+)(?:.*?)(?:,|\band\b|\bthen\b)?\s*(?:show|find|give|get|what|which)", 
         "given X, show Y"),
        
        # Pattern: "for the X, find/show Y"
        (r"for\s+(?:the\s+)?(?:top|best|largest|highest|biggest)\s+(\d+\s+)?(\w+)(?:.*?)(?:,|\band\b|\bthen\b)?\s*(?:show|find|give|get|what|which)",
         "for X, find Y"),
        
        # Pattern: "within X, show Y"
        (r"within\s+(?:the\s+)?(?:top|best|largest|highest|biggest)?\s*(\d+\s+)?(\w+)(?:.*?)(?:,|\band\b|\bthen\b)?\s*(?:show|find|give|get|what|which)",
         "within X, show Y"),
        
        # Pattern: "in X, what/which are Y"
        (r"in\s+(?:the\s+)?(?:top|best|largest|highest|biggest)?\s*(\d+\s+)?(\w+)(?:.*?)(?:,|\band\b|\bthen\b)?\s*(?:show|find|give|get|what|which)",
         "in X, what are Y"),
        
        # Pattern: "first X, then Y"
        (r"first\s+(?:find|get|show|the)?\s*(?:top|best|largest|highest|biggest)?\s*(\d+\s+)?(\w+)(?:.*?)(?:,|\band\b)?\s*(?:then|after|next)",
         "first X, then Y"),
    ]
    
    # Check if any hierarchical pattern is present
    import re
    hierarchical_detected = False
    for pattern, pattern_name in hierarchical_patterns:
        if re.search(pattern, user_query_lower):
            hierarchical_detected = True
            logger.info(f"Detected hierarchical filter pattern: '{pattern_name}' in user query")
            break
    
    # If no hierarchical pattern detected, return query as-is
    if not hierarchical_detected:
        return query_draft
    
    # Get filters from query
    filters = query_draft.get("query", {}).get("filters", [])
    
    # If we don't have multiple filters, no context needed
    if len(filters) < 2:
        return query_draft
    
    # Apply context logic:
    # 1. If we have 2 filters and hierarchical pattern detected:
    #    - First filter gets context:true
    #    - Second filter operates within that context
    # 
    # 2. Priority for context filters (which should be applied first):
    #    a) SET filters (specific value filters)
    #    b) DATE filters (time ranges)
    #    c) TOP filters with lower howMany (broader scope)
    #    d) QUANTITATIVE filters
    
    modified = False
    
    # Check if any filter already has context:true
    has_context = any(f.get("context") for f in filters)
    
    if not has_context and len(filters) == 2:
        # Simple case: 2 filters, no context set yet
        first_filter = filters[0]
        
        # Apply context to first filter if it's a valid type
        filter_type = first_filter.get("filterType", "")
        if filter_type in ["SET", "TOP", "QUANTITATIVE_NUMERICAL", "DATE"]:
            first_filter["context"] = True
            logger.warning(
                f"ðŸ”§ AUTO-FIX: Applied context:true to first filter (type: {filter_type}) "
                f"due to hierarchical pattern detected in: '{user_query}'"
            )
            modified = True
    
    elif not has_context and len(filters) > 2:
        # Complex case: 3+ filters
        # Heuristic: Apply context to broader scope filters (SET, DATE, or TOP with smaller howMany)
        
        # Find SET or DATE filters first (they typically define broad scopes)
        for i, f in enumerate(filters):
            filter_type = f.get("filterType", "")
            if filter_type in ["SET", "DATE"] and not f.get("context"):
                f["context"] = True
                logger.warning(
                    f"ðŸ”§ AUTO-FIX: Applied context:true to {filter_type} filter at position {i} "
                    f"due to hierarchical pattern detected in: '{user_query}'"
                )
                modified = True
                break
        
        # If no SET/DATE found, look for TOP filters and apply context to the one with smaller howMany
        if not modified:
            top_filters = [(i, f) for i, f in enumerate(filters) if f.get("filterType") == "TOP"]
            if len(top_filters) >= 2:
                # Sort by howMany (ascending), smaller N = broader scope
                top_filters_sorted = sorted(top_filters, key=lambda x: x[1].get("howMany", 999))
                # Apply context to the first (smallest N)
                idx, first_top = top_filters_sorted[0]
                first_top["context"] = True
                logger.warning(
                    f"ðŸ”§ AUTO-FIX: Applied context:true to TOP filter with howMany={first_top.get('howMany')} "
                    f"due to hierarchical pattern detected in: '{user_query}'"
                )
                modified = True
    
    if modified:
        logger.info(f"Applied automatic context filter fixes to query")
    
    return query_draft


def _adjust_calculated_field_names(query_draft: Dict[str, Any], enriched_schema: Optional[Dict[str, Any]] = None, schema: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    """
    Adjust calculated field names to avoid conflicts with existing schema fields.
    
    If a calculated field has a fieldCaption that matches an existing field name,
    automatically rename it to avoid conflicts.
    
    Args:
        query_draft: The query generated by the LLM
        enriched_schema: Enriched schema with field_map
        schema: Basic schema with columns
        
    Returns:
        Modified query with adjusted calculated field names
    """
    # Build set of existing field names (case-insensitive)
    existing_fields: Set[str] = set()
    
    if enriched_schema and isinstance(enriched_schema, dict):
        field_map = enriched_schema.get("field_map", {})
        for field_lower, field_info in field_map.items():
            field_caption = field_info.get("fieldCaption", "")
            if field_caption:
                existing_fields.add(field_caption.lower())
    elif schema and isinstance(schema, dict):
        for col in schema.get("columns", []):
            field_name = col.get("name", "")
            if field_name:
                existing_fields.add(field_name.lower())
    
    if not existing_fields:
        # No schema available, can't check conflicts
        return query_draft
    
    fields = query_draft.get("query", {}).get("fields", [])
    modified = False
    
    for field in fields:
        # Only check fields with calculations
        if "calculation" not in field:
            continue
        
        field_caption = field.get("fieldCaption", "")
        if not field_caption:
            continue
        
        field_lower = field_caption.lower()
        
        # Check if field name conflicts with existing field
        if field_lower in existing_fields:
            # Generate a new name by appending " (Calculated)" or similar suffix
            # Try common suffixes in order of preference
            suffixes = [" (Calculated)", " Calculated", " Ratio", " Margin", " Rate"]
            new_name = None
            
            for suffix in suffixes:
                candidate = field_caption + suffix
                if candidate.lower() not in existing_fields:
                    new_name = candidate
                    break
            
            # If all suffixes conflict, try numbered version
            if not new_name:
                counter = 1
                while True:
                    candidate = f"{field_caption} (Calculated {counter})"
                    if candidate.lower() not in existing_fields:
                        new_name = candidate
                        break
                    counter += 1
                    if counter > 100:  # Safety limit
                        logger.warning(f"Could not find unique name for calculated field '{field_caption}'")
                        break
            
            if new_name:
                logger.warning(
                    f"ðŸ”§ AUTO-FIX: Renamed calculated field '{field_caption}' to '{new_name}' "
                    f"to avoid conflict with existing schema field"
                )
                field["fieldCaption"] = new_name
                modified = True
    
    if modified:
        logger.info("Applied automatic calculated field name adjustments to query")
    
    return query_draft


def _create_tool_functions(datasource_id: str, site_id: Optional[str], message_history: List[Dict]) -> List[Dict]:
    """Create tool function definitions for LLM."""
    return [
        {
            "name": "get_datasource_schema",
            "description": "Fetch datasource schema (columns, measures, dimensions). Use this if schema is not provided in state or you need fresh schema data.",
            "parameters": {
                "type": "object",
                "properties": {
                    "datasource_id": {
                        "type": "string",
                        "description": "Datasource LUID"
                    },
                    "use_enriched": {
                        "type": "boolean",
                        "description": "Whether to use enriched schema with semantic metadata",
                        "default": True
                    }
                },
                "required": ["datasource_id"]
            }
        },
        {
            "name": "get_datasource_metadata",
            "description": "Fetch datasource metadata via REST API (name, project, tags, certification). Use this when you need datasource context or description.",
            "parameters": {
                "type": "object",
                "properties": {
                    "datasource_id": {
                        "type": "string",
                        "description": "Datasource LUID"
                    },
                    "site_id": {
                        "type": "string",
                        "description": "Site ID (optional)"
                    }
                },
                "required": ["datasource_id"]
            }
        },
        {
            "name": "get_prior_query",
            "description": "Search conversation history for similar queries. Use this when user query is similar to a prior message - you can reuse and modify the prior query instead of building from scratch.",
            "parameters": {
                "type": "object",
                "properties": {
                    "current_query": {
                        "type": "string",
                        "description": "Current user query"
                    },
                    "similarity_threshold": {
                        "type": "number",
                        "description": "Similarity threshold (0.0-1.0), default 0.8",
                        "default": 0.8
                    }
                },
                "required": ["current_query"]
            }
        }
    ]


async def _execute_tool_call(tool_name: str, args: Dict[str, Any], state: StreamlinedVizQLState) -> Dict[str, Any]:
    """Execute a tool call."""
    datasource_id = state.get("context_datasources", [None])[0]
    site_id = state.get("site_id")
    message_history = state.get("messages", [])
    
    try:
        if tool_name == "get_datasource_schema":
            schema_result = await get_datasource_schema(
                datasource_id=args.get("datasource_id", datasource_id),
                site_id=args.get("site_id", site_id),
                use_enriched=args.get("use_enriched", True)
            )
            return {"success": True, "result": schema_result}
        
        elif tool_name == "get_datasource_metadata":
            metadata_result = await get_datasource_metadata(
                datasource_id=args.get("datasource_id", datasource_id),
                site_id=args.get("site_id", site_id)
            )
            return {"success": True, "result": metadata_result}
        
        elif tool_name == "get_prior_query":
            prior_query_result = get_prior_query(
                message_history=message_history,
                current_query=args.get("current_query", state.get("user_query", "")),
                similarity_threshold=args.get("similarity_threshold", 0.8)
            )
            return {"success": True, "result": prior_query_result}
        
        else:
            return {"success": False, "error": f"Unknown tool: {tool_name}"}
    
    except Exception as e:
        logger.error(f"Error executing tool {tool_name}: {e}", exc_info=True)
        return {"success": False, "error": str(e)}


@track_node_execution("vizql_streamlined", "query_builder")
async def build_query_node(state: StreamlinedVizQLState) -> Dict[str, Any]:
    """
    Intelligently construct VizQL query with tool support.
    
    This node can:
    - Reuse queries from conversation history
    - Fetch schema only if needed
    - Fetch datasource metadata for context
    - Make intelligent decisions about what information is needed
    """
    try:
        datasource_ids = state.get("context_datasources", [])
        if not datasource_ids:
            return {
                **state,
                "error": "No datasource ID available.",
                "query_draft": None
            }
        
        datasource_id = datasource_ids[0]
        user_query = state.get("user_query", "")
        message_history = state.get("messages", [])
        enriched_schema = state.get("enriched_schema")
        schema = state.get("schema")
        attempt = state.get("attempt", 1)
        validation_errors = state.get("validation_errors", [])
        execution_errors = state.get("execution_errors", [])
        
        # Increment attempt if this is a retry
        if validation_errors or execution_errors:
            attempt = attempt + 1
        
        # Initialize reasoning steps
        reasoning_steps = state.get("reasoning_steps", [])
        reasoning_steps.append({
            "node": "build_query",
            "timestamp": datetime.utcnow().isoformat(),
            "thought": f"Building query for: {user_query}",
            "attempt": attempt
        })
        
        # Get API key and model
        api_key = state.get("api_key")
        model = state.get("model", "gpt-4")
        
        if not api_key:
            logger.error("API key missing from state")
            return {
                **state,
                "error": "Failed to build query: Authorization header required",
                "query_draft": None,
                "reasoning_steps": reasoning_steps
            }
        
        # Step 1: Extract context from conversation history
        prior_query_result = None
        context_fields = []  # Fields from previous queries
        context_measures = []  # Measures from previous queries
        context_dimensions = []  # Dimensions from previous queries
        
        if message_history:
            # Check for similar queries (for reuse)
            prior_query_result = get_prior_query(
                message_history=message_history,
                current_query=user_query,
                similarity_threshold=0.8
            )
            
            # Also extract fields from most recent assistant message (even if not similar)
            # This handles cases like "break it down by region" after "show me sales"
            user_query_lower = user_query.lower()
            reference_keywords = ["break", "break down", "break it down", "by", "for each", "those", "that", "it", "them", "group"]
            has_reference = any(keyword in user_query_lower for keyword in reference_keywords)
            
            if has_reference:
                # Look for most recent assistant message with a query
                for msg in reversed(message_history):
                    if msg.get("role") == "assistant":
                        query_draft = msg.get("query_draft")
                        if query_draft and isinstance(query_draft, dict):
                            prev_fields = query_draft.get("query", {}).get("fields", [])
                            for field in prev_fields:
                                field_caption = field.get("fieldCaption", "")
                                if field_caption:
                                    # Determine if measure or dimension based on function presence
                                    if field.get("function"):
                                        if field_caption not in context_measures:
                                            context_measures.append(field_caption)
                                    else:
                                        if field_caption not in context_dimensions:
                                            context_dimensions.append(field_caption)
                                    if field_caption not in context_fields:
                                        context_fields.append(field_caption)
                            logger.info(
                                f"Extracted context fields from previous query: "
                                f"{len(context_fields)} fields ({len(context_measures)} measures, {len(context_dimensions)} dimensions)"
                            )
                            break
        
        # Step 2: Determine if we need to fetch schema
        needs_schema = not schema and not enriched_schema
        
        # CRITICAL: Ensure we have schema before building query
        # Without schema, LLM cannot know which fields are available
        if needs_schema:
            logger.info("Schema not available - fetching before building query")
            try:
                schema_result = await get_datasource_schema(
                    datasource_id=datasource_id,
                    site_id=state.get("site_id"),
                    use_enriched=True
                )
                if schema_result and not schema_result.get("error"):
                    schema = {
                        "columns": schema_result.get("columns", [])
                    }
                    if schema_result.get("enriched"):
                        # Use the full enriched schema structure (includes fields array)
                        enriched_schema = {
                            "fields": schema_result.get("fields", []),  # Full field objects
                            "measures": schema_result.get("measures", []),
                            "dimensions": schema_result.get("dimensions", []),
                            "field_map": schema_result.get("field_map", {}),
                            "datasource_id": schema_result.get("datasource_id")
                        }
                    reasoning_steps.append({
                        "node": "build_query",
                        "timestamp": datetime.utcnow().isoformat(),
                        "action": "tool_call",
                        "tool": "get_datasource_schema",
                        "result": "success",
                        "fields_count": len(schema_result.get("columns", []))
                    })
                    logger.info(f"Fetched schema with {len(schema.get('columns', []))} columns")
                else:
                    error_msg = f"Failed to fetch schema: {schema_result.get('error', 'Unknown error')}"
                    logger.error(error_msg)
                    return {
                        **state,
                        "error": error_msg,
                        "query_draft": None,
                        "reasoning_steps": reasoning_steps
                    }
            except Exception as e:
                logger.error(f"Error fetching schema: {e}", exc_info=True)
                return {
                    **state,
                    "error": f"Failed to fetch schema: {str(e)}",
                    "query_draft": None,
                    "reasoning_steps": reasoning_steps
                }
        
        # Final check: ensure we have schema
        if not schema and not enriched_schema:
            error_msg = "Cannot build query without schema. Schema fetch failed or returned empty."
            logger.error(error_msg)
            return {
                **state,
                "error": error_msg,
                "query_draft": None,
                "reasoning_steps": reasoning_steps
            }
        
        # Step 3: Build prompt with available context
        # If we have enriched schema, use compressed context
        if enriched_schema:
            field_count = len(enriched_schema.get("fields", []))
            measure_count = len(enriched_schema.get("measures", []))
            dimension_count = len(enriched_schema.get("dimensions", []))
            logger.info(
                f"Using enriched schema with compressed context: "
                f"{field_count} fields ({measure_count} measures, {dimension_count} dimensions)"
            )
            # Only pass intent parsing results if they're actually set and relevant
            # Since we removed the planner node, these should be empty unless explicitly set
            # Don't pass calculations/bins unless user explicitly requested them
            required_measures = state.get("required_measures", [])
            required_dimensions = state.get("required_dimensions", [])
            required_filters = state.get("required_filters", {})
            topN = state.get("topN", {"enabled": False})
            sorting = state.get("sorting", [])
            calculations = state.get("calculations", [])
            bins = state.get("bins", [])
            
            # CRITICAL: Only include calculations/bins if user explicitly mentioned them in query
            # Check if user query mentions calculation-related terms
            user_query_lower = user_query.lower()
            calculation_keywords = ["calculate", "calculation", "formula", "ratio", "margin", "percentage", "divide", "divided by"]
            bin_keywords = ["bin", "bins", "bucket", "buckets", "group by range"]
            
            # Filter out calculations/bins that weren't explicitly requested
            if calculations:
                # Only keep calculations if user query mentions calculation-related terms
                if not any(keyword in user_query_lower for keyword in calculation_keywords):
                    logger.info(f"Ignoring {len(calculations)} calculations from state - not mentioned in user query")
                    calculations = []
            
            if bins:
                # Only keep bins if user query mentions bin-related terms
                if not any(keyword in user_query_lower for keyword in bin_keywords):
                    logger.info(f"Ignoring {len(bins)} bins from state - not mentioned in user query")
                    bins = []
            
            compressed_context = build_full_compressed_context(
                enriched_schema=enriched_schema,
                user_query=user_query,
                required_measures=required_measures if required_measures else None,
                required_dimensions=required_dimensions if required_dimensions else None,
                required_filters=required_filters if required_filters else None,
                topN=topN if topN.get("enabled") else None,
                sorting=sorting if sorting else None,
                calculations=calculations if calculations else None,
                bins=bins if bins else None
            )
            
            # Split compressed context
            context_lines = compressed_context.split("\n")
            compressed_schema_lines = []
            semantic_hints_lines = []
            field_lookup_lines = []
            parsed_intent_lines = []
            current_section = None
            
            for line in context_lines:
                if line.startswith("## Available Fields"):
                    current_section = "schema"
                    compressed_schema_lines.append(line)
                elif line.startswith("## Query Construction Hints"):
                    current_section = "hints"
                    semantic_hints_lines.append(line)
                elif line.startswith("## Field Matching Hints"):
                    current_section = "lookup"
                    field_lookup_lines.append(line)
                elif line.startswith("## Parsed Intent"):
                    current_section = "intent"
                    parsed_intent_lines.append(line)
                elif current_section == "schema":
                    compressed_schema_lines.append(line)
                elif current_section == "hints":
                    semantic_hints_lines.append(line)
                elif current_section == "lookup":
                    field_lookup_lines.append(line)
                elif current_section == "intent":
                    parsed_intent_lines.append(line)
            
            compressed_schema = "\n".join(compressed_schema_lines) if compressed_schema_lines else ""
            semantic_hints = "\n".join(semantic_hints_lines) if semantic_hints_lines else ""
            field_lookup_hints = "\n".join(field_lookup_lines) if field_lookup_lines else ""
            parsed_intent = "\n".join(parsed_intent_lines) if parsed_intent_lines else ""
            
            system_prompt = prompt_registry.get_prompt(
                "agents/vizql/query_construction.txt",
                variables={
                    "compressed_schema": compressed_schema,
                    "semantic_hints": semantic_hints,
                    "field_lookup_hints": field_lookup_hints,
                    "parsed_intent": parsed_intent,
                    "datasource_id": datasource_id
                }
            )
        elif schema:
            # Use basic schema
            column_count = len(schema.get("columns", []))
            logger.info(f"Using basic schema with {column_count} columns")
            system_prompt = prompt_registry.get_prompt(
                "agents/vizql/query_construction.txt",
                variables={
                    "compressed_schema": f"## Available Fields\n{json.dumps(schema.get('columns', []), indent=2)}",
                    "semantic_hints": "## Query Construction Hints\nUsing basic schema. Field roles may not be available.",
                    "field_lookup_hints": "",
                    "parsed_intent": "",
                    "datasource_id": datasource_id
                }
            )
        else:
            # This should not happen - schema should be fetched above
            error_msg = "Schema not available after fetch attempt. Cannot build query."
            logger.error(error_msg)
            return {
                **state,
                "error": error_msg,
                "query_draft": None,
                "reasoning_steps": reasoning_steps
            }
        
        # Add retry context if this is a retry
        if attempt > 1:
            retry_context = "\n\n## Previous Attempt Failed\n"
            if validation_errors:
                retry_context += f"Validation Errors:\n" + "\n".join(f"- {e}" for e in validation_errors) + "\n"
            if execution_errors:
                retry_context += f"Execution Errors:\n" + "\n".join(f"- {e}" for e in execution_errors) + "\n"
            retry_context += "Fix these issues in your query."
            system_prompt += retry_context
        
        # Add conversation context
        conversation_context_parts = []
        
        if prior_query_result:
            conversation_context_parts.append(f"## Similar Query Found in History\n")
            conversation_context_parts.append(f"Previous query (similarity: {prior_query_result['similarity_score']:.2f}):\n")
            conversation_context_parts.append(f"User message: {prior_query_result['message']}\n")
            conversation_context_parts.append(f"Query: {json.dumps(prior_query_result['query'], indent=2)}\n")
            conversation_context_parts.append("You can reuse and modify this query instead of building from scratch.")
        
        # Add context fields if user query references previous messages
        if context_fields:
            conversation_context_parts.append(f"\n## Fields from Previous Query\n")
            conversation_context_parts.append(f"**CRITICAL**: The user's current query references a previous query.")
            if context_measures:
                conversation_context_parts.append(f"**Measures from previous query**: {', '.join(context_measures)}")
                conversation_context_parts.append(f"**YOU MUST INCLUDE THESE MEASURES** in your current query!")
            if context_dimensions:
                conversation_context_parts.append(f"**Dimensions from previous query**: {', '.join(context_dimensions)}")
            conversation_context_parts.append(f"\n**Example**: If previous query used 'Sales' and user says 'break it down by region',")
            conversation_context_parts.append(f"you MUST include BOTH 'Sales' (from previous) AND 'Region' (new dimension) in query.fields!")
        
        if conversation_context_parts:
            system_prompt += "\n\n" + "\n".join(conversation_context_parts)
        
        # Build messages with conversation history context
        examples = prompt_registry.get_examples("agents/vizql/examples.yaml")
        messages = prompt_registry.build_few_shot_prompt(
            system_prompt,
            examples,
            f"Build query for: {user_query}"
        )
        
        # Add conversation history context before the current query
        # This helps the LLM understand references like "it", "that", "break it down"
        if message_history and len(message_history) > 0:
            # Extract relevant context from message history (all messages, no limit)
            conversation_context = []
            for i, msg in enumerate(message_history):
                role = msg.get("role", "user")
                content = msg.get("content", "")
                
                if role == "user":
                    conversation_context.append(f"User: {content}")
                elif role == "assistant":
                    # Include assistant response and any query metadata
                    assistant_content = content
                    if msg.get("query_draft"):
                        assistant_content += f"\n[Previous query used: {json.dumps(msg['query_draft'], indent=2)}]"
                    if msg.get("query_results"):
                        results = msg["query_results"]
                        columns = results.get("columns", [])
                        row_count = results.get("row_count", 0)
                        assistant_content += f"\n[Previous query returned {row_count} rows with columns: {', '.join(columns)}]"
                    conversation_context.append(f"Assistant: {assistant_content}")
            
            if conversation_context:
                # Insert conversation history before the current user query
                history_text = "\n\n## Conversation History\n" + "\n\n".join(conversation_context)
                history_text += "\n\n**IMPORTANT**: Use the conversation history above to understand context. "
                history_text += "If the user says 'break it down by region' after asking about 'sales', "
                history_text += "they want to break down the SALES metric (from the previous query) by REGION."
                history_text += "\n\nIf previous queries used specific fields, reuse those fields in your query."
                
                # Add history context to the last user message
                if messages and messages[-1].get("role") == "user":
                    messages[-1]["content"] = history_text + "\n\n" + messages[-1]["content"]
                else:
                    # Add as separate message if structure is different
                    messages.insert(-1, {"role": "user", "content": history_text})
        
        # Schema should already be fetched above if needed
        
        # Track tool calls for this step
        tool_calls_made = []
        if enriched_schema:
            tool_calls_made.append("get_datasource_schema")
        if state.get("datasource_metadata"):
            tool_calls_made.append("get_datasource_metadata")
        if state.get("query_reused"):
            tool_calls_made.append("get_prior_query")
        
        # Call LLM to build query
        ai_client = UnifiedAIClient(
            gateway_url=settings.GATEWAY_BASE_URL,
            api_key=api_key
        )
        
        response = await ai_client.chat(
            model=model,
            messages=messages,
            api_key=api_key
        )
        
        # Track token usage
        tokens_used = {
            "prompt": response.prompt_tokens,
            "completion": response.completion_tokens,
            "total": response.tokens_used
        }
        
        # Parse query JSON
        try:
            content = response.content.strip()
            # Remove markdown code blocks if present
            if content.startswith("```"):
                lines = content.split("\n")
                json_start = None
                json_end = None
                for i, line in enumerate(lines):
                    if "```json" in line.lower() or "```" in line:
                        if json_start is None:
                            json_start = i + 1
                        else:
                            json_end = i
                            break
                if json_start and json_end:
                    content = "\n".join(lines[json_start:json_end])
                elif json_start:
                    content = "\n".join(lines[json_start:-1])
            
            # Try to extract JSON if there's extra content after it
            # Look for the first complete JSON object
            query_draft = None
            
            # First try: parse the entire content
            try:
                query_draft = json.loads(content)
            except json.JSONDecodeError:
                # Second try: find JSON object boundaries
                # Look for first { and matching }
                start_idx = content.find('{')
                if start_idx != -1:
                    brace_count = 0
                    end_idx = start_idx
                    for i in range(start_idx, len(content)):
                        if content[i] == '{':
                            brace_count += 1
                        elif content[i] == '}':
                            brace_count -= 1
                            if brace_count == 0:
                                end_idx = i + 1
                                break
                    
                    if end_idx > start_idx:
                        json_str = content[start_idx:end_idx]
                        try:
                            query_draft = json.loads(json_str)
                            logger.info(f"Extracted JSON from response (char {start_idx} to {end_idx})")
                        except json.JSONDecodeError:
                            pass
            
            # Third try: look for JSON array if object didn't work
            if query_draft is None:
                start_idx = content.find('[')
                if start_idx != -1:
                    bracket_count = 0
                    end_idx = start_idx
                    for i in range(start_idx, len(content)):
                        if content[i] == '[':
                            bracket_count += 1
                        elif content[i] == ']':
                            bracket_count -= 1
                            if bracket_count == 0:
                                end_idx = i + 1
                                break
                    
                    if end_idx > start_idx:
                        json_str = content[start_idx:end_idx]
                        try:
                            query_draft = json.loads(json_str)
                            logger.info(f"Extracted JSON array from response")
                        except json.JSONDecodeError:
                            pass
            
            if query_draft is None:
                # Log the problematic content for debugging
                logger.error(f"Could not extract JSON from response. Content preview: {content[:500]}")
                raise json.JSONDecodeError("Could not extract valid JSON from response", content, 0)
            
            # Ensure query has required structure
            if "datasource" not in query_draft:
                query_draft["datasource"] = {"datasourceLuid": datasource_id}
            if "query" not in query_draft:
                query_draft["query"] = {}
            if "options" not in query_draft:
                query_draft["options"] = {
                    "returnFormat": "OBJECTS",
                    "disaggregate": False
                }
            
            # CRITICAL: Ensure fields array exists and is populated
            if "fields" not in query_draft.get("query", {}):
                query_draft["query"]["fields"] = []
            
            # Validate that fields array is not empty
            fields = query_draft.get("query", {}).get("fields", [])
            if not fields or len(fields) == 0:
                error_msg = (
                    "Query must have at least one field in query.fields array. "
                    f"Schema available: {bool(schema or enriched_schema)}. "
                    f"User query: {user_query}"
                )
                logger.error(error_msg)
                logger.error(f"Query draft structure: {json.dumps(query_draft, indent=2)}")
                raise ValueError(error_msg)
            
            # CRITICAL: Apply automatic date function detection and correction
            # This ensures date fields have proper temporal functions even if LLM missed them
            query_draft = _detect_and_apply_date_functions(query_draft, user_query, enriched_schema)
            
            # CRITICAL: Apply automatic COUNTD detection for "how many" queries
            # This ensures count queries have COUNTD function even if LLM missed it
            query_draft = _detect_and_apply_count_functions(query_draft, user_query, enriched_schema)
            
            # CRITICAL: Apply automatic context filter detection
            # This ensures hierarchical filter dependencies are properly handled
            query_draft = _detect_and_apply_context_filters(query_draft, user_query)
            
            # CRITICAL: Adjust calculated field names to avoid conflicts with existing fields
            # This ensures calculated fields have unique names even if LLM missed the instruction
            query_draft = _adjust_calculated_field_names(query_draft, enriched_schema, schema)
            
            # Log fields being added for debugging (after auto-fix)
            fields = query_draft.get("query", {}).get("fields", [])  # Refresh after auto-fix
            field_names = [f.get("fieldCaption", "unknown") for f in fields]
            logger.info(f"Query includes {len(fields)} fields: {field_names}")
            
            # Check if fields match user query intent (including context fields from conversation)
            user_query_lower = user_query.lower()
            user_mentioned_fields = []
            for field_name in field_names:
                field_lower = field_name.lower()
                # Check if field name or parts of it are mentioned in user query OR in context
                if (field_lower in user_query_lower or 
                    any(word in user_query_lower for word in field_lower.split()) or
                    field_name in context_fields):
                    user_mentioned_fields.append(field_name)
            
            if len(user_mentioned_fields) < len(field_names):
                extra_fields = [f for f in field_names if f not in user_mentioned_fields]
                if context_fields:
                    logger.info(
                        f"Query includes fields from conversation context: {context_fields}. "
                        f"Extra fields: {extra_fields}"
                    )
                else:
                    logger.warning(
                        f"Query includes fields not explicitly mentioned in user query: {extra_fields}. "
                        f"User query: '{user_query}'. "
                        f"Only include fields the user explicitly requested!"
                    )
            
            query_version = state.get("query_version", 0)
            if query_version == 0:
                query_version = 1
            
            reasoning_steps.append({
                "node": "build_query",
                "timestamp": datetime.utcnow().isoformat(),
                "action": "query_built",
                "query_reused": prior_query_result is not None,
                "fields_count": len(query_draft.get("query", {}).get("fields", []))
            })
            
            return {
                **state,
                "query_draft": query_draft,
                "query_version": query_version,
                "schema": schema,
                "enriched_schema": enriched_schema if enriched_schema else state.get("enriched_schema"),
                "query_reused": prior_query_result is not None,
                "reasoning": response.content,
                "reasoning_steps": reasoning_steps,
                "attempt": attempt,  # Include updated attempt
                "current_thought": f"Built query version {query_version} with {len(query_draft.get('query', {}).get('fields', []))} fields",
                "step_metadata": {
                    "tool_calls": tool_calls_made,
                    "tokens": tokens_used
                }
            }
            
        except (json.JSONDecodeError, ValueError) as e:
            # Handle JSON parsing errors and validation errors
            error_msg = str(e)
            response_preview = response.content[:1000] if response.content else "No content"
            
            # Check if it's a fields validation error
            if "fields" in error_msg.lower() or "field" in error_msg.lower() or isinstance(e, ValueError):
                error_msg = (
                    f"Query validation failed: {error_msg}. "
                    "The LLM did not include any fields in the query. "
                    "This may happen if the schema is missing or the user query is unclear."
                )
                logger.error(f"Query validation error: {error_msg}")
            else:
                # JSON parsing error
                logger.error(f"Failed to parse query JSON: {e}")
                logger.error(f"Response preview (first 1000 chars): {response_preview}")
                logger.error(f"Full response length: {len(response.content) if response.content else 0} chars")
                error_msg = f"Failed to parse query JSON: {error_msg}. The LLM response may contain extra text after the JSON. Check logs for full response."
            
            reasoning_steps.append({
                "node": "build_query",
                "timestamp": datetime.utcnow().isoformat(),
                "action": "error",
                "error": error_msg,
                "response_preview": response_preview if isinstance(e, json.JSONDecodeError) else None
            })
            return {
                **state,
                "error": error_msg,
                "query_draft": None,
                "reasoning_steps": reasoning_steps
            }
        except Exception as e:
            logger.error(f"Error building query: {e}", exc_info=True)
            reasoning_steps.append({
                "node": "build_query",
                "timestamp": datetime.utcnow().isoformat(),
                "action": "error",
                "error": str(e)
            })
            return {
                **state,
                "error": f"Failed to build query: {str(e)}",
                "query_draft": None,
                "reasoning_steps": reasoning_steps
            }
    except Exception as e:
        # Catch any exceptions that escape the inner try blocks
        logger.error(f"Unexpected error in build_query_node: {e}", exc_info=True)
        reasoning_steps = state.get("reasoning_steps", [])
        reasoning_steps.append({
            "node": "build_query",
            "timestamp": datetime.utcnow().isoformat(),
            "action": "error",
            "error": f"Unexpected error: {str(e)}"
        })
        return {
            **state,
            "error": f"Failed to build query: {str(e)}",
            "query_draft": None,
            "reasoning_steps": reasoning_steps
        }
